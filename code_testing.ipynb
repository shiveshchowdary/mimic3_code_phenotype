{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab8033-8bd2-419f-9fcd-e97849e90046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "pd.set_option('future.no_silent_downcasting',True)\n",
    "\n",
    "class MimicDataSetPhenotype(Dataset):\n",
    "    def __init__(self, data_dir, csv_file, mean_variance , cat_dict, mode, seq_len, pad_value = 0, device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.csv_file = csv_file\n",
    "        self.seq_len = seq_len\n",
    "        self.mode = mode\n",
    "        self.data_df = pd.read_csv(csv_file)\n",
    "        self.mean_variance = mean_variance\n",
    "        self.pad_value = pad_value\n",
    "        self.device = device\n",
    "        self.cat_dict = cat_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data_dir + self.data_df['stay'][idx]\n",
    "        data = pd.read_csv(path)\n",
    "        # categorical_variables = ['Glascow coma scale eye opening', \n",
    "        #                          'Glascow coma scale motor response', \n",
    "        #                          'Glascow coma scale verbal response']\n",
    "        id_name_dict = {}\n",
    "        # data.drop(labels=categorical_variables, axis=1, inplace=True)\n",
    "        data.replace(['ERROR','no data','.','-','/','VERIFIED','CLOTTED',\"*\",'ERROR DISREGARD PREVIOUS RESULT OF 32','DISREGARD PREVIOUSLY REPORTED 33'], np.nan, inplace=True)\n",
    "        for i in range(len(data.columns)):\n",
    "            id_name_dict[i] = data.columns[i]\n",
    "        values = data.values\n",
    "        sample = self.extract(values, id_name_dict)\n",
    "        if len(sample[0]) >= self.seq_len :\n",
    "            sample[0] = sample[0][-self.seq_len:]\n",
    "            sample[1] = sample[1][-self.seq_len:]\n",
    "            sample[2] = sample[2][-self.seq_len:]\n",
    "            sample[3] = sample[3][-self.seq_len:]\n",
    "        num_padd_tokens = self.seq_len - len(sample[0])\n",
    "        \n",
    "        variable_input = torch.cat([\n",
    "            torch.tensor(sample[2], dtype=torch.int64),\n",
    "            torch.tensor([self.pad_value]*num_padd_tokens, dtype=torch.int64)\n",
    "        ])\n",
    "        value_input = torch.cat([\n",
    "            torch.tensor(sample[1], dtype=torch.float),\n",
    "            torch.tensor([self.pad_value]*num_padd_tokens, dtype=torch.float)\n",
    "        ])\n",
    "        val = torch.tensor(sample[0], dtype=torch.float)\n",
    "        time_input = torch.cat([\n",
    "             val - val.min() ,\n",
    "            torch.tensor([self.pad_value]*num_padd_tokens, dtype=torch.float)\n",
    "        ])\n",
    "        variables = sample[3] + ['pad token']*num_padd_tokens\n",
    "        \n",
    "        assert variable_input.size(0) == self.seq_len\n",
    "        assert value_input.size(0) == self.seq_len\n",
    "        assert time_input.size(0) == self.seq_len\n",
    "        cols = self.data_df.columns[2:]\n",
    "        return {\n",
    "            \"encoder_input\" : [time_input.to(self.device), variable_input.to(self.device), value_input.to(self.device)],\n",
    "            \"encoder_mask\": (variable_input != self.pad_value).unsqueeze(0).int().to(self.device),\n",
    "            \"variables\" : variables,\n",
    "            \"label\" : torch.tensor(self.data_df[cols].values[idx].argmax(), dtype=torch.int64).to(self.device)\n",
    "        }\n",
    "    \n",
    "    def extract(self, values, id_name_dict):\n",
    "        sample = [[],[],[],[]]\n",
    "        for i in range(values.shape[0]):\n",
    "            time = values[i,0]\n",
    "            for j in range(1, values.shape[1]):\n",
    "                if self.isNAN(values[i][j]) == False:\n",
    "                    if id_name_dict[j] in self.cat_dict.keys():\n",
    "                        sample[0].append(time)\n",
    "                        sample[1].append(self.cat_dict[id_name_dict[j]][values[i][j]])\n",
    "                        sample[2].append(j)\n",
    "                        sample[3].append(id_name_dict[j])\n",
    "                    else:\n",
    "                        mean = self.mean_variance[id_name_dict[j]]['mean']\n",
    "                        var = self.mean_variance[id_name_dict[j]]['variance']\n",
    "                        val = (float(values[i][j]) - mean)/var\n",
    "                        sample[0].append(time)\n",
    "                        sample[1].append(val)\n",
    "                        sample[2].append(j)\n",
    "                        sample[3].append(id_name_dict[j])\n",
    "        return sample\n",
    "    def isNAN(self, val):\n",
    "        return val!=val\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_roc_auc(model, data_loader):\n",
    "    model.eval()\n",
    "    all_probabilities = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(data_loader, leave=False):\n",
    "            outputs = model(inputs['encoder_input'], inputs['encoder_mask'])\n",
    "            labels = inputs['label']\n",
    "            logits = torch.sigmoid(outputs)\n",
    "            \n",
    "            all_probabilities.append(logits.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    logits_all = np.concatenate(all_probabilities)\n",
    "    labels_all = np.concatenate(all_labels)\n",
    "    \n",
    "    roc_auc = roc_auc_score(labels_all, logits_all)\n",
    "    return roc_auc\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "def calculate_auc_prc(model, data_loader):\n",
    "    model.eval()\n",
    "    all_probabilities = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(data_loader, leave=False):\n",
    "            outputs = model(inputs['encoder_input'], inputs['encoder_mask'])\n",
    "            labels = inputs['label']\n",
    "            logits = torch.sigmoid(outputs)\n",
    "\n",
    "            all_probabilities.append(logits.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    logits_all = np.concatenate(all_probabilities)\n",
    "    labels_all = np.concatenate(all_labels)\n",
    "\n",
    "    auc_prc = average_precision_score(labels_all, logits_all)\n",
    "    return auc_prc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4aa404-1fc3-4b38-bea7-62dcad52f328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "pd.set_option('future.no_silent_downcasting',True)\n",
    "\n",
    "class Normalizer:\n",
    "    def __init__(self, data, data_dir):\n",
    "        self.data = data\n",
    "        self.data_dir = data_dir\n",
    "        self.categorical_variables = ['Glascow coma scale eye opening', \n",
    "                                 'Glascow coma scale motor response', \n",
    "                                 'Glascow coma scale verbal response']\n",
    "        self.mean_var_dict = self.get_mean_var()\n",
    "        \n",
    "        \n",
    "    def get_mean_var(self):\n",
    "        sample_path = self.data_dir + self.data['stay'][0]\n",
    "        id_name_dict = {}\n",
    "        df = pd.read_csv(sample_path)\n",
    "        df.drop(labels=self.categorical_variables, axis=1, inplace=True)\n",
    "        for i in range(len(df.columns)):\n",
    "            id_name_dict[i] = df.columns[i]\n",
    "        variable_values = {k : [] for k in df.columns[1:]}\n",
    "        for sample_path in tqdm(self.data['stay']):\n",
    "            sample_path = self.data_dir+sample_path\n",
    "            df = pd.read_csv(sample_path)\n",
    "            values = df.values\n",
    "            df.drop(labels=self.categorical_variables, axis=1, inplace=True)\n",
    "            df.replace(['ERROR','no data','.','-','/','VERIFIED','CLOTTED',\"*\",'ERROR DISREGARD PREVIOUS RESULT OF 32','DISREGARD PREVIOUSLY REPORTED 33'], np.nan, inplace=True)\n",
    "            cols = df.columns[1:]\n",
    "            df = df[cols]\n",
    "            values = df.values\n",
    "            for i in range(values.shape[0]):\n",
    "                for j in range(values.shape[1]):\n",
    "                    if self.isNAN(values[i][j]) == False:\n",
    "                        variable_values[id_name_dict[j+1]].append((float(values[i][j])))\n",
    "        result_dict = {}\n",
    "        for feature, values in variable_values.items():\n",
    "            mean_value = np.mean(values)\n",
    "            variance_value = np.var(values)\n",
    "            result_dict[feature] = {'mean': mean_value, 'variance': variance_value}\n",
    "        return result_dict\n",
    "    def isNAN(self, val):\n",
    "        return val!=val\n",
    "    \n",
    "\n",
    "train_data_path = \"/data/datasets/mimic3_18var/root/phenotyping/train_listfile.csv\"\n",
    "val_data_path = \"/data/datasets/mimic3_18var/root/phenotyping/val_listfile.csv\"\n",
    "\n",
    "data_dir = \"/data/datasets/mimic3_18var/root/phenotyping/train/\"\n",
    "\n",
    "\n",
    "save = False\n",
    "if save:\n",
    "    normalizer = Normalizer(pd.read_csv(train_data_path), data_dir)\n",
    "    with open('normalizer.pkl', 'wb') as file:\n",
    "        pickle.dump(normalizer, file)\n",
    "\n",
    "    print(\"Completed Saving Normalizer........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96a83e-b654-4cd5-85fc-45b25e192d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer(pd.read_csv(train_data_path), data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a78dbba-b1d4-41ac-9bf4-9da6b66fc827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalizer.mean_var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751cbe7-f8de-4ae1-bad8-5c9c96574ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "pd.set_option('future.no_silent_downcasting',True)\n",
    "\n",
    "\n",
    "category_config = {\n",
    "    \"Glascow coma scale verbal response\": {\n",
    "            \"No Response-ETT\": 1,\n",
    "            \"No Response\": 1,\n",
    "            \"1 No Response\": 1,\n",
    "            \"1.0 ET/Trach\": 1,\n",
    "            \"2 Incomp sounds\": 2,\n",
    "            \"Incomprehensible sounds\": 2,\n",
    "            \"3 Inapprop words\": 3,\n",
    "            \"Inappropriate Words\": 3,\n",
    "            \"4 Confused\": 4,\n",
    "            \"Confused\": 4,\n",
    "            \"5 Oriented\": 5,\n",
    "            \"Oriented\": 5\n",
    "    },\n",
    "    \"Glascow coma scale eye opening\": {\n",
    "            \"None\": 0,\n",
    "            \"1 No Response\": 1,\n",
    "            \"2 To pain\": 2, \n",
    "            \"To Pain\": 2,\n",
    "            \"3 To speech\": 3, \n",
    "            \"To Speech\": 3,\n",
    "            \"4 Spontaneously\": 4,\n",
    "            \"Spontaneously\": 4\n",
    "        },\n",
    "    \"Glascow coma scale motor response\": {\n",
    "            \"1 No Response\": 1,\n",
    "            \"No response\": 1,\n",
    "            \"2 Abnorm extensn\": 2,\n",
    "            \"Abnormal extension\": 2,\n",
    "            \"3 Abnorm flexion\": 3,\n",
    "            \"Abnormal Flexion\": 3,\n",
    "            \"4 Flex-withdraws\": 4,\n",
    "            \"Flex-withdraws\": 4,\n",
    "            \"5 Localizes Pain\": 5,\n",
    "            \"Localizes Pain\": 5,\n",
    "            \"6 Obeys Commands\": 6,\n",
    "            \"Obeys Commands\": 6\n",
    "        }\n",
    "}\n",
    "\n",
    "class Categorizer:\n",
    "    def __init__(self, data, data_dir):\n",
    "        self.category_dict = category_config\n",
    "        self.data = data\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "train_data_path = \"/data/datasets/mimic3_18var/root/phenotyping/train_listfile.csv\"\n",
    "val_data_path = \"/data/datasets/mimic3_18var/root/phenotyping/val_listfile.csv\"\n",
    "\n",
    "data_dir = \"/data/datasets/mimic3_18var/root/phenotyping/train/\"\n",
    "\n",
    "\n",
    "save = False\n",
    "\n",
    "if save:\n",
    "    categorizer = Categorizer(pd.read_csv(train_data_path), data_dir)\n",
    "    with open(\"categorizer.pkl\", \"wb\") as file:\n",
    "        pickle.dump(categorizer, file)\n",
    "\n",
    "    print(\"Completed Saving Categorizer........\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2888300c-9736-46f1-8912-780ac18b7b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorizer = Categorizer(pd.read_csv(train_data_path), data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf40f9d-f4f2-479d-ba90-aa4d7141cc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = MimicDataSetPhenotype(data_dir, train_data_path, normalizer.mean_var_dict, categorizer.category_dict, 'training', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e0846-20f3-42dd-8fbe-2d494b1fc5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_ds, batch_size = 32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a38fde-fd4f-4cbb-aa4d-be4d29e811b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds.data_df[cols].values[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4485d-62bb-4801-980b-20cbc79ee12a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba0cc1-8d7e-4bd3-b1d4-e0c15a6c30da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['encoder_input'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d099dabf-e20f-4b58-b57c-67ed47f4df46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['encoder_input'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10585702-b8ab-4179-b836-273c453578ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['encoder_input'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585ee84-ed49-42dc-9b7c-ffa794cac711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3dcad4-87be-4a4e-b3da-ec6d500eca93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch['label'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b0e5b-417f-4939-9d27-34366c843ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols = (train_ds.data_df.columns[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a11011-001c-49d4-b1c4-17a2554eb219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789c818-73df-412e-be17-967f5a6ae0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds.data_df[cols].values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f17bb-b98c-4b7b-a8ac-dc321fbe0f83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_ds = MimicDataSetPhenotype(data_dir, val_data_path, normalizer.mean_var_dict, categorizer.category_dict, 'validation', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8145e7-b1ed-4b5b-971e-215b2a74e9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_ds, batch_size = 32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758f55d-df6e-4ca2-9594-2adeb816c8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in tqdm(val_dataloader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df96bc50-8116-46f8-aad0-67a0c421a43a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 77186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal number of parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mimic/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mimic/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/mimic/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/mimic/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/mimic/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/code_phenotype/utils.py:40\u001b[0m, in \u001b[0;36mMimicDataSetPhenotype.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m     id_name_dict[i] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[1;32m     39\u001b[0m values \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m---> 40\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_name_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sample[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len :\n\u001b[1;32m     42\u001b[0m     sample[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len:]\n",
      "File \u001b[0;32m~/code_phenotype/utils.py:79\u001b[0m, in \u001b[0;36mMimicDataSetPhenotype.extract\u001b[0;34m(self, values, id_name_dict)\u001b[0m\n\u001b[1;32m     77\u001b[0m time \u001b[38;5;241m=\u001b[39m values[i,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNAN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m id_name_dict[j] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     81\u001b[0m             sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(time)\n",
      "File \u001b[0;32m~/code_phenotype/utils.py:94\u001b[0m, in \u001b[0;36mMimicDataSetPhenotype.isNAN\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     sample[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(id_name_dict[j])\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misNAN\u001b[39m(\u001b[38;5;28mself\u001b[39m, val):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\u001b[38;5;241m!=\u001b[39mval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_LEN = 448\n",
    "batch_size = 32\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "N = 2\n",
    "num_variables = 18 \n",
    "num_variables += 1 #for no variable embedding while doing padding\n",
    "d_ff = 128\n",
    "epochs = 100\n",
    "learning_rate = 8e-4\n",
    "drop_out = 0.2\n",
    "sinusoidal = True\n",
    "th_val_roc = 0.84\n",
    "th_val_pr = 0.48\n",
    "num_classes = 25\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import MimicDataSetPhenotype, calculate_multi_class_metrics\n",
    "pd.set_option('future.no_silent_downcasting',True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from model import Model\n",
    "from tqdm import tqdm\n",
    "from normalizer import Normalizer\n",
    "from categorizer import Categorizer\n",
    "\n",
    "\n",
    "train_data_path = \"/data/datasets/mimic3_18var/root/phenotyping/train_listfile.csv\"\n",
    "val_data_path = \"/data/datasets/mimic3_18var/root/phenotyping/val_listfile.csv\"\n",
    "\n",
    "data_dir = \"/data/datasets/mimic3_18var/root/phenotyping/train/\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('normalizer.pkl', 'rb') as file:\n",
    "    normalizer = pickle.load(file)\n",
    "\n",
    "with open('categorizer.pkl', 'rb') as file:\n",
    "    categorizer = pickle.load(file)\n",
    "    \n",
    "\n",
    "mean_variance = normalizer.mean_var_dict\n",
    "cat_dict = categorizer.category_dict\n",
    "\n",
    "\n",
    "train_ds = MimicDataSetPhenotype(data_dir, train_data_path, mean_variance, cat_dict, 'training', MAX_LEN)\n",
    "val_ds = MimicDataSetPhenotype(data_dir, val_data_path, mean_variance, cat_dict, 'validation', MAX_LEN)\n",
    "# test_ds = MimicDataSetPhenotype(test_data_dir, test_data_path, mean_variance, cat_dict,'testing', MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size = batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_ds, batch_size = 1, shuffle=True)\n",
    "\n",
    "model = Model(d_model, num_heads, d_ff, num_classes, N, sinusoidal).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {total_params}')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}', leave=False):\n",
    "        inp = batch['encoder_input']\n",
    "        mask = batch['encoder_mask']\n",
    "        y = batch['label']\n",
    "        outputs = model(inp, mask)\n",
    "        loss = criterion(outputs, y.float().reshape(-1,1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    roc_auc_micro, roc_auc_macro = calculate_multi_class_metrics(model, val_dataloader)\n",
    "    # print(f'Epoch {epoch + 1}/{epochs}, Train AUC-ROC: {calculate_roc_auc(model, train_dataloader):.3f}')\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Validation Micro AUC-ROC: {roc_auc_micro:.3f}')\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Validation Macro AUC-ROC: {roc_auc_macro:.3f}')\n",
    "    if (auc_prc > th_val_pr) or (auc_roc > th_val_roc):\n",
    "        print(\"Reached threshold limit stopping...............\")\n",
    "        break\n",
    "\n",
    "# print(\"Testing...............\")\n",
    "# print(f\"Validation AUC-ROC, AUC-PRC: {calculate_roc_auc(model, test_dataloader):.3f}, {calculate_auc_prc(model, test_dataloader):.3f}\")\n",
    "\n",
    "# Constructing the file path\n",
    "file_path = f\"model_maxlen{MAX_LEN}_batch{batch_size}_dmodel{d_model}_heads{num_heads}_N{N}_vars{num_variables}_dff{d_ff}_epochs{epochs}_lr{learning_rate}_dropout{drop_out}_sinusoidal{sinusoidal}_testing.pth\"\n",
    "\n",
    "# Example usage\n",
    "torch.save(model.state_dict(), \"models/\"+ file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0cf828-aac1-49e6-ab0a-cfe3088dd5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf77f8f-aedc-4fd8-8c78-58fd773f3b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "mimic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
